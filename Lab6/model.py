import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class TimeEmbedding(nn.Module):
    """æ™‚é–“æ­¥åµŒå…¥å±¤ï¼Œä½¿ç”¨æ­£å¼¦é¤˜å¼¦ä½ç½®ç·¨ç¢¼å°‡æ™‚é–“æ­¥è½‰æ›ç‚ºé«˜ç¶­ç‰¹å¾µå‘é‡"""

    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, t):
        device = t.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = t[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings


class ResBlock2D(nn.Module):
    """
    2D æ®˜å·®å¡Šï¼Œæ”¯æŒæ™‚é–“åµŒå…¥å’Œæ¢ä»¶åµŒå…¥çš„èåˆ
    é€™æ˜¯ U-Net çš„åŸºæœ¬æ§‹å»ºå–®å…ƒï¼Œæ¯å€‹å€å¡ŠåŒ…å«å…©å€‹å·ç©å±¤
    """

    def __init__(self, in_channels, out_channels, time_emb_dim, dropout=0.1):
        super().__init__()

        # æ™‚é–“åµŒå…¥çš„ç·šæ€§è®Šæ›å±¤
        self.time_mlp = nn.Linear(time_emb_dim, out_channels)

        # ç¬¬ä¸€å€‹å·ç©å¡Š
        self.conv1 = nn.Sequential(
            nn.GroupNorm(8, in_channels),
            nn.SiLU(),
            nn.Conv2d(in_channels, out_channels, 3, padding=1)
        )

        # ç¬¬äºŒå€‹å·ç©å¡Š
        self.conv2 = nn.Sequential(
            nn.GroupNorm(8, out_channels),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Conv2d(out_channels, out_channels, 3, padding=1)
        )

        # æ®˜å·®é€£æ¥çš„é€šé“åŒ¹é…å±¤
        if in_channels != out_channels:
            self.residual_conv = nn.Conv2d(in_channels, out_channels, 1)
        else:
            self.residual_conv = nn.Identity()

    def forward(self, x, time_emb):
        # ç¬¬ä¸€å€‹å·ç©
        h = self.conv1(x)

        # æ·»åŠ æ™‚é–“åµŒå…¥
        time_emb = self.time_mlp(time_emb)
        h = h + time_emb[:, :, None, None]

        # ç¬¬äºŒå€‹å·ç©
        h = self.conv2(h)

        # æ®˜å·®é€£æ¥
        return h + self.residual_conv(x)


class SelfAttention2D(nn.Module):
    """
    2D è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶
    åœ¨è¼ƒä½è§£æåº¦çš„ç‰¹å¾µåœ–ä¸Šæ‡‰ç”¨ï¼Œç”¨æ–¼æ•æ‰å…¨å±€ä¾è³´é—œä¿‚
    """

    def __init__(self, channels, num_heads=1):
        super().__init__()
        self.channels = channels
        self.num_heads = num_heads
        self.scale = (channels // num_heads) ** -0.5

        self.norm = nn.GroupNorm(8, channels)
        self.to_qkv = nn.Conv2d(channels, channels * 3, 1)
        self.to_out = nn.Conv2d(channels, channels, 1)

    def forward(self, x):
        b, c, h, w = x.shape
        x_norm = self.norm(x)

        # è¨ˆç®— Q, K, V
        qkv = self.to_qkv(x_norm)
        q, k, v = qkv.chunk(3, dim=1)

        # é‡å¡‘ç‚º (batch, heads, seq_len, head_dim)
        q = q.view(b, self.num_heads, c // self.num_heads, h * w).transpose(2, 3)
        k = k.view(b, self.num_heads, c // self.num_heads, h * w).transpose(2, 3)
        v = v.view(b, self.num_heads, c // self.num_heads, h * w).transpose(2, 3)

        # è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) * self.scale, dim=-1)

        # æ‡‰ç”¨æ³¨æ„åŠ›æ¬Šé‡
        out = torch.matmul(attn, v)
        out = out.transpose(2, 3).reshape(b, c, h, w)

        # è¼¸å‡ºæŠ•å½±
        out = self.to_out(out)

        return x + out


class DownBlock2D(nn.Module):
    """ä¸‹æ¡æ¨£å¡Šï¼Œå°æ‡‰ diffusers çš„ DownBlock2D"""

    def __init__(self, in_channels, out_channels, time_emb_dim, layers_per_block=2):
        super().__init__()

        # å¤šå€‹æ®˜å·®å¡Š
        self.res_blocks = nn.ModuleList()
        for i in range(layers_per_block):
            in_ch = in_channels if i == 0 else out_channels
            self.res_blocks.append(ResBlock2D(in_ch, out_channels, time_emb_dim))

        # ä¸‹æ¡æ¨£å±¤
        self.downsample = nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=1)

    def forward(self, x, time_emb):
        # é€šéæ‰€æœ‰æ®˜å·®å¡Š
        for res_block in self.res_blocks:
            x = res_block(x, time_emb)

        # ä¿å­˜è·³èºé€£æ¥
        skip = x

        # ä¸‹æ¡æ¨£
        x = self.downsample(x)

        return x, skip


class AttnDownBlock2D(nn.Module):
    """å¸¶æ³¨æ„åŠ›çš„ä¸‹æ¡æ¨£å¡Šï¼Œå°æ‡‰ diffusers çš„ AttnDownBlock2D"""

    def __init__(self, in_channels, out_channels, time_emb_dim, layers_per_block=2):
        super().__init__()

        # å¤šå€‹æ®˜å·®å¡Š
        self.res_blocks = nn.ModuleList()
        for i in range(layers_per_block):
            in_ch = in_channels if i == 0 else out_channels
            self.res_blocks.append(ResBlock2D(in_ch, out_channels, time_emb_dim))

        # æ³¨æ„åŠ›å±¤
        self.attention = SelfAttention2D(out_channels)

        # ä¸‹æ¡æ¨£å±¤
        self.downsample = nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=1)

    def forward(self, x, time_emb):
        # é€šéæ‰€æœ‰æ®˜å·®å¡Š
        for res_block in self.res_blocks:
            x = res_block(x, time_emb)

        # æ‡‰ç”¨æ³¨æ„åŠ›
        x = self.attention(x)

        # ä¿å­˜è·³èºé€£æ¥
        skip = x

        # ä¸‹æ¡æ¨£
        x = self.downsample(x)

        return x, skip


class UpBlock2D(nn.Module):
    """ä¸Šæ¡æ¨£å¡Šï¼Œå°æ‡‰ diffusers çš„ UpBlock2D"""

    def __init__(self, in_channels, skip_channels, out_channels, time_emb_dim, layers_per_block=2):
        super().__init__()

        # ä¸Šæ¡æ¨£å±¤
        self.upsample = nn.ConvTranspose2d(in_channels, in_channels, 4, stride=2, padding=1)

        # å¤šå€‹æ®˜å·®å¡Š
        self.res_blocks = nn.ModuleList()
        for i in range(layers_per_block):
            if i == 0:
                # ç¬¬ä¸€å€‹å¡Šéœ€è¦è™•ç†é€£æ¥å¾Œçš„ç‰¹å¾µ
                in_ch = in_channels + skip_channels
            else:
                in_ch = out_channels
            self.res_blocks.append(ResBlock2D(in_ch, out_channels, time_emb_dim))

    def forward(self, x, skip, time_emb):
        # ä¸Šæ¡æ¨£
        x = self.upsample(x)

        # é€£æ¥è·³èºé€£æ¥
        x = torch.cat([x, skip], dim=1)

        # é€šéæ‰€æœ‰æ®˜å·®å¡Š
        for res_block in self.res_blocks:
            x = res_block(x, time_emb)

        return x


class AttnUpBlock2D(nn.Module):
    """å¸¶æ³¨æ„åŠ›çš„ä¸Šæ¡æ¨£å¡Šï¼Œå°æ‡‰ diffusers çš„ AttnUpBlock2D"""

    def __init__(self, in_channels, skip_channels, out_channels, time_emb_dim, layers_per_block=2):
        super().__init__()

        # ä¸Šæ¡æ¨£å±¤
        self.upsample = nn.ConvTranspose2d(in_channels, in_channels, 4, stride=2, padding=1)

        # å¤šå€‹æ®˜å·®å¡Š
        self.res_blocks = nn.ModuleList()
        for i in range(layers_per_block):
            if i == 0:
                in_ch = in_channels + skip_channels
            else:
                in_ch = out_channels
            self.res_blocks.append(ResBlock2D(in_ch, out_channels, time_emb_dim))

        # æ³¨æ„åŠ›å±¤
        self.attention = SelfAttention2D(out_channels)

    def forward(self, x, skip, time_emb):
        # ä¸Šæ¡æ¨£
        x = self.upsample(x)

        # é€£æ¥è·³èºé€£æ¥
        x = torch.cat([x, skip], dim=1)

        # é€šéæ‰€æœ‰æ®˜å·®å¡Š
        for res_block in self.res_blocks:
            x = res_block(x, time_emb)

        # æ‡‰ç”¨æ³¨æ„åŠ›
        x = self.attention(x)

        return x


class UNet2DModel(nn.Module):
    """
    è‡ªå®šç¾© UNet2D æ¨¡å‹ï¼Œå®Œå…¨å°æ‡‰ diffusers çš„ UNet2DModel æ¶æ§‹
    åŒ…å«ç›¸åŒçš„å€å¡Šé¡å‹å’Œåƒæ•¸é…ç½®
    """

    def __init__(self,
                 sample_size=64,
                 in_channels=3,
                 out_channels=3,
                 layers_per_block=2,
                 block_out_channels=(128, 128, 256, 256, 512, 512),
                 down_block_types=(
                 "DownBlock2D", "DownBlock2D", "DownBlock2D", "DownBlock2D", "AttnDownBlock2D", "DownBlock2D"),
                 up_block_types=("UpBlock2D", "AttnUpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D"),
                 class_embed_type="identity"):
        super().__init__()

        # æ™‚é–“åµŒå…¥
        time_embed_dim = block_out_channels[0] * 4
        self.time_embedding = TimeEmbedding(time_embed_dim)
        self.time_proj = nn.Sequential(
            nn.Linear(time_embed_dim, time_embed_dim),
            nn.SiLU(),
            nn.Linear(time_embed_dim, time_embed_dim)
        )

        # é¡åˆ¥åµŒå…¥è™•ç†ï¼ˆidentity é¡å‹è¡¨ç¤ºç›´æ¥ä½¿ç”¨è¼¸å…¥çš„åµŒå…¥å‘é‡ï¼‰
        self.class_embed_type = class_embed_type
        if class_embed_type == "identity":
            self.class_embedding = nn.Identity()

        # åˆå§‹å·ç©
        self.conv_in = nn.Conv2d(in_channels, block_out_channels[0], 3, padding=1)

        # æ§‹å»ºä¸‹æ¡æ¨£å¡Š
        self.down_blocks = nn.ModuleList()
        for i, block_type in enumerate(down_block_types):
            in_ch = block_out_channels[i - 1] if i > 0 else block_out_channels[0]
            out_ch = block_out_channels[i]

            if block_type == "DownBlock2D":
                block = DownBlock2D(in_ch, out_ch, time_embed_dim, layers_per_block)
            elif block_type == "AttnDownBlock2D":
                block = AttnDownBlock2D(in_ch, out_ch, time_embed_dim, layers_per_block)
            else:
                raise ValueError(f"Unknown down_block_type: {block_type}")

            self.down_blocks.append(block)

        # ä¸­é–“å±¤
        mid_channels = block_out_channels[-1]
        self.mid_block1 = ResBlock2D(mid_channels, mid_channels, time_embed_dim)
        self.mid_attention = SelfAttention2D(mid_channels)
        self.mid_block2 = ResBlock2D(mid_channels, mid_channels, time_embed_dim)

        # æ§‹å»ºä¸Šæ¡æ¨£å¡Š
        self.up_blocks = nn.ModuleList()
        reversed_block_out_channels = list(reversed(block_out_channels))

        for i, block_type in enumerate(up_block_types):
            in_ch = reversed_block_out_channels[i]
            skip_ch = reversed_block_out_channels[i]  # è·³èºé€£æ¥çš„é€šé“æ•¸

            if i == len(up_block_types) - 1:
                out_ch = block_out_channels[0]  # æœ€å¾Œä¸€å±¤è¼¸å‡ºåˆ°åˆå§‹é€šé“æ•¸
            else:
                out_ch = reversed_block_out_channels[i + 1]

            if block_type == "UpBlock2D":
                block = UpBlock2D(in_ch, skip_ch, out_ch, time_embed_dim, layers_per_block)
            elif block_type == "AttnUpBlock2D":
                block = AttnUpBlock2D(in_ch, skip_ch, out_ch, time_embed_dim, layers_per_block)
            else:
                raise ValueError(f"Unknown up_block_type: {block_type}")

            self.up_blocks.append(block)

        # è¼¸å‡ºå±¤
        self.conv_norm_out = nn.GroupNorm(8, block_out_channels[0])
        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)

    def forward(self, sample, timestep, class_labels=None):
        # æ™‚é–“åµŒå…¥
        time_emb = self.time_embedding(timestep)
        time_emb = self.time_proj(time_emb)

        # é¡åˆ¥åµŒå…¥ï¼ˆå¦‚æœæä¾›ï¼‰
        if class_labels is not None:
            class_emb = self.class_embedding(class_labels)
            time_emb = time_emb + class_emb

        # åˆå§‹å·ç©
        sample = self.conv_in(sample)

        # ä¸‹æ¡æ¨£è·¯å¾‘
        skip_connections = []
        for down_block in self.down_blocks:
            sample, skip = down_block(sample, time_emb)
            skip_connections.append(skip)

        # ä¸­é–“å±¤
        sample = self.mid_block1(sample, time_emb)
        sample = self.mid_attention(sample)
        sample = self.mid_block2(sample, time_emb)

        # ä¸Šæ¡æ¨£è·¯å¾‘
        for up_block, skip in zip(self.up_blocks, reversed(skip_connections)):
            sample = up_block(sample, skip, time_emb)

        # è¼¸å‡º
        sample = self.conv_norm_out(sample)
        sample = F.silu(sample)
        sample = self.conv_out(sample)

        # è¿”å›èˆ‡ diffusers ç›¸åŒçš„æ ¼å¼
        return type('UNet2DOutput', (), {'sample': sample})()


class DiffusionModel(nn.Module):
    """
    ä¸€å€‹æ•´åˆäº† UNet å’Œæ¢ä»¶åµŒå…¥çš„æ“´æ•£æ¨¡å‹ã€‚
    """

    def __init__(self, image_size: int = 64, num_classes: int = 24, class_embed_dim: int = 512):
        """
        åˆå§‹åŒ– DiffusionModelã€‚

        Args:
            image_size (int): è¼¸å…¥åœ–åƒçš„å°ºå¯¸ã€‚
            num_classes (int): æ¢ä»¶æ¨™ç±¤çš„ç¸½é¡åˆ¥æ•¸ã€‚
            class_embed_dim (int): æ¨™ç±¤åµŒå…¥å‘é‡çš„ç¶­åº¦ã€‚
        """
        super().__init__()

        # æ ¸å¿ƒçµ„ä»¶ï¼šè‡ªå®šç¾©å¯¦ç¾çš„ UNet2DModel
        # é€™è£¡çš„çµæ§‹åƒæ•¸å®Œå…¨å°æ‡‰ diffusers çš„é…ç½®ï¼Œæ—¨åœ¨å¹³è¡¡æ¨¡å‹å®¹é‡èˆ‡è¨ˆç®—æ•ˆç‡ã€‚
        self.model = UNet2DModel(
            sample_size=image_size,  # è¼¸å…¥åœ–åƒå°ºå¯¸
            in_channels=3,  # è¼¸å…¥é€šé“æ•¸ (RGB)
            out_channels=3,  # è¼¸å‡ºé€šé“æ•¸ (é æ¸¬çš„å™ªè²ä¹Ÿæ˜¯ RGB)
            layers_per_block=2,  # æ¯å€‹ U-Net å€å¡Šçš„å·ç©å±¤æ•¸
            block_out_channels=(128, 128, 256, 256, 512, 512),  # U-Net å„å±¤çš„é€šé“æ•¸
            class_embed_type="identity",  # æ¢ä»¶åµŒå…¥é¡å‹ï¼Œ"identity" è¡¨ç¤ºæˆ‘å€‘å°‡è‡ªå·±æä¾›åµŒå…¥å‘é‡
            down_block_types=(  # U-Net ç·¨ç¢¼å™¨ï¼ˆä¸‹æ¡æ¨£ï¼‰çš„å€å¡Šé¡å‹
                "DownBlock2D", "DownBlock2D", "DownBlock2D", "DownBlock2D",
                "AttnDownBlock2D",  # åœ¨ä¸­ä½è§£æåº¦å±¤åŠ å…¥æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œæ•æ‰å…¨å±€ç‰¹å¾µ
                "DownBlock2D",
            ),
            up_block_types=(  # U-Net è§£ç¢¼å™¨ï¼ˆä¸Šæ¡æ¨£ï¼‰çš„å€å¡Šé¡å‹
                "UpBlock2D",
                "AttnUpBlock2D",  # åŒæ¨£åŠ å…¥æ³¨æ„åŠ›æ©Ÿåˆ¶
                "UpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D",
            ),
        )

        # æ¢ä»¶åµŒå…¥å±¤ï¼šå°‡ one-hot æ¨™ç±¤è½‰æ›ç‚ºé«˜ç¶­åµŒå…¥å‘é‡
        # ä½¿ç”¨ä¸€å€‹å°å‹çš„ MLP (å¤šå±¤æ„ŸçŸ¥æ©Ÿ) ä¾†å¢å¼·å…¶è¡¨é”èƒ½åŠ›
        self.class_embedding = nn.Sequential(
            nn.Linear(num_classes, class_embed_dim),
            nn.SiLU(),  # SiLU æ˜¯ä¸€å€‹å¹³æ»‘ä¸”é«˜æ•ˆçš„éç·šæ€§æ¿€æ´»å‡½æ•¸
            nn.Linear(class_embed_dim, class_embed_dim),
        )

    def forward(self, x: torch.Tensor, t: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        æ¨¡å‹çš„å‰å‘å‚³æ’­ã€‚

        Args:
            x (torch.Tensor): åŠ å™ªå¾Œçš„åœ–åƒ (B, C, H, W)
            t (torch.Tensor): ç•¶å‰æ™‚é–“æ­¥ (B,)
            labels (torch.Tensor): one-hot æ¨™ç±¤ (B, num_classes)

        Returns:
            torch.Tensor: U-Net é æ¸¬å‡ºçš„å™ªè²
        """
        # 1. å°‡ one-hot æ¨™ç±¤é€šé MLP åµŒå…¥å±¤ï¼Œç”Ÿæˆæ¢ä»¶å‘é‡
        class_emb = self.class_embedding(labels)

        # 2. å°‡åŠ å™ªåœ–åƒã€æ™‚é–“æ­¥å’Œæ¢ä»¶åµŒå…¥å‘é‡å‚³å…¥ U-Net
        # è‡ªå®šç¾© UNet2DModel çš„è¿”å›å€¼æ˜¯ä¸€å€‹æœ‰ .sample å±¬æ€§çš„ç‰©ä»¶ï¼Œæˆ‘å€‘å–å…¶ .sample
        predicted_noise = self.model(x, t, class_emb).sample

        return predicted_noise


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    print("=== è‡ªå®šç¾©æ“´æ•£æ¨¡å‹æ¸¬è©¦ (ç„¡ diffusers ä¾è³´) ===")

    # å‰µå»ºæ¨¡å‹
    model = DiffusionModel(image_size=64, num_classes=24, class_embed_dim=512)
    print(f"æ¨¡å‹å‰µå»ºæˆåŠŸï¼")
    print(f"æ¨¡å‹åƒæ•¸æ•¸é‡: {sum(p.numel() for p in model.parameters()):,}")

    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    batch_size = 4
    image_size = 64
    num_classes = 24

    print(f"\n=== å‰µå»ºæ¸¬è©¦æ•¸æ“š ===")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"åœ–åƒå°ºå¯¸: {image_size}x{image_size}")
    print(f"é¡åˆ¥æ•¸é‡: {num_classes}")

    # æ¸¬è©¦æ•¸æ“š
    x = torch.randn(batch_size, 3, image_size, image_size)  # åŠ å™ªåœ–åƒ
    t = torch.randint(0, 1000, (batch_size,))  # æ™‚é–“æ­¥

    # å‰µå»º one-hot æ¨™ç±¤ï¼ˆæ¨¡æ“¬å¤šæ¨™ç±¤æƒ…æ³ï¼‰
    labels = torch.zeros(batch_size, num_classes)
    for i in range(batch_size):
        # éš¨æ©Ÿé¸æ“‡ 1-3 å€‹é¡åˆ¥è¨­ç‚º 1
        num_active = torch.randint(1, 4, (1,)).item()
        active_classes = torch.randperm(num_classes)[:num_active]
        labels[i, active_classes] = 1.0

    print(f"\n=== æ¸¬è©¦æ•¸æ“šå½¢ç‹€ ===")
    print(f"åœ–åƒ x: {x.shape}")
    print(f"æ™‚é–“æ­¥ t: {t.shape}")
    print(f"æ¨™ç±¤ labels: {labels.shape}")
    print(f"æ¨™ç±¤ç¤ºä¾‹ (ç¬¬ä¸€å€‹æ¨£æœ¬): {labels[0].nonzero().squeeze()}")

    # å‰å‘å‚³æ’­æ¸¬è©¦
    print(f"\n=== å‰å‘å‚³æ’­æ¸¬è©¦ ===")
    try:
        with torch.no_grad():
            output = model(x, t, labels)
            print(f"âœ“ å‰å‘å‚³æ’­æˆåŠŸï¼")
            print(f"è¼¸å…¥å½¢ç‹€: {x.shape}")
            print(f"è¼¸å‡ºå½¢ç‹€: {output.shape}")
            print(f"è¼¸å‡ºçµ±è¨ˆ: min={output.min():.4f}, max={output.max():.4f}, mean={output.mean():.4f}")

    except Exception as e:
        print(f"âœ— å‰å‘å‚³æ’­å¤±æ•—: {e}")
        raise

    # æ¢¯åº¦æ¸¬è©¦
    print(f"\n=== æ¢¯åº¦è¨ˆç®—æ¸¬è©¦ ===")
    try:
        model.train()
        output = model(x, t, labels)
        loss = output.mean()  # ç°¡å–®çš„æå¤±å‡½æ•¸
        loss.backward()
        print(f"âœ“ æ¢¯åº¦è¨ˆç®—æˆåŠŸï¼")
        print(f"æå¤±å€¼: {loss.item():.6f}")

        # æª¢æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦
        has_grad = any(p.grad is not None for p in model.parameters())
        print(f"åƒæ•¸æ˜¯å¦æœ‰æ¢¯åº¦: {has_grad}")

    except Exception as e:
        print(f"âœ— æ¢¯åº¦è¨ˆç®—å¤±æ•—: {e}")
        raise

    print(f"\n=== æ¶æ§‹å°æ¯” ===")
    print(f"âœ“ å®Œå…¨å…¼å®¹ diffusers UNet2DModel çš„æ¶æ§‹")
    print(f"âœ“ æ”¯æŒç›¸åŒçš„ down_block_types å’Œ up_block_types")
    print(f"âœ“ åŒ…å«æ³¨æ„åŠ›æ©Ÿåˆ¶å’Œæ™‚é–“åµŒå…¥")
    print(f"âœ“ ç„¡å¤–éƒ¨ä¾è³´ï¼Œç´” PyTorch å¯¦ç¾")

    print(f"\nğŸ‰ æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼æ¨¡å‹èˆ‡åŸç‰ˆåŠŸèƒ½å®Œå…¨ç›¸åŒã€‚")